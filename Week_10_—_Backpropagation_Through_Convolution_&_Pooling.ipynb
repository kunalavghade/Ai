{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNE6Ki5g8m0ohFm/E5KwWUK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalavghade/Ai/blob/main/Week_10_%E2%80%94_Backpropagation_Through_Convolution_%26_Pooling.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 10 â€” Backpropagation Through Convolution & Pooling\n",
        "\n",
        "## Goal\n",
        "\n",
        "Implement backward propagation for:\n",
        "- Convolution layer\n",
        "- ReLU\n",
        "- Max pooling\n",
        "\n",
        "Then train a small CNN on MNIST (subset).\n",
        "\n",
        "By the end of this week, you should:\n",
        "- Understand convolution gradients mathematically\n",
        "- Implement conv backward pass\n",
        "- Implement max-pooling backward pass\n",
        "- Train CNN end-to-end\n",
        "- Verify gradients numerically\n",
        "\n",
        "This is the hardest week of Phase 3.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Why Backprop Through Convolution Is Hard\n",
        "\n",
        "In fully connected layers:\n",
        "- Gradients are matrix multiplications.\n",
        "\n",
        "In convolution:\n",
        "- Each input pixel affects multiple outputs.\n",
        "- Filters are reused spatially.\n",
        "- Gradients must be accumulated carefully.\n",
        "\n",
        "Backprop becomes structured accumulation.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Forward Recap\n",
        "\n",
        "For one filter:\n",
        "\n",
        "Output(i,j) = Î£ Î£ Input(i+m, j+n) * Kernel(m,n)\n",
        "\n",
        "Now we need:\n",
        "\n",
        "âˆ‚L/âˆ‚Kernel  \n",
        "âˆ‚L/âˆ‚Input  \n",
        "\n",
        "Given âˆ‚L/âˆ‚Output.\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Gradient w.r.t Kernel\n",
        "\n",
        "Each kernel weight affects many output locations.\n",
        "\n",
        "Formula intuition:\n",
        "\n",
        "dK(m,n) =\n",
        "    Î£ Î£ dOutput(i,j) * Input(i+m, j+n)\n",
        "\n",
        "Meaning:\n",
        "- Slide input under gradient map.\n",
        "- Accumulate contributions.\n",
        "\n",
        "This is cross-correlation with upstream gradient.\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Gradient w.r.t Input\n",
        "\n",
        "Each input pixel contributes to multiple outputs.\n",
        "\n",
        "Formula intuition:\n",
        "\n",
        "dInput =\n",
        "    Convolution(dOutput, rotated_kernel)\n",
        "\n",
        "Rotate kernel 180Â° before applying.\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Handling Padding in Backward\n",
        "\n",
        "If forward used padding:\n",
        "- Backward must remove padding\n",
        "- Gradients must match original input shape\n",
        "\n",
        "Always check:\n",
        "- Shape consistency\n",
        "\n",
        "---\n",
        "\n",
        "# 6. ReLU Backward\n",
        "\n",
        "Simple:\n",
        "\n",
        "If input > 0 â†’ gradient passes  \n",
        "Else â†’ gradient = 0\n",
        "\n",
        "---\n",
        "\n",
        "# 7. Max Pooling Backward\n",
        "\n",
        "Key idea:\n",
        "- Only the max element receives gradient.\n",
        "- Others get zero.\n",
        "\n",
        "Store during forward:\n",
        "- Argmax indices per window\n",
        "\n",
        "Backward:\n",
        "- Route gradient to stored location.\n",
        "\n",
        "---\n",
        "\n",
        "# 8. Full Backward Flow\n",
        "\n",
        "For CNN block:\n",
        "\n",
        "Conv â†’ ReLU â†’ Pool\n",
        "\n",
        "Backward order:\n",
        "\n",
        "Pool backward  \n",
        "â†’ ReLU backward  \n",
        "â†’ Conv backward  \n",
        "\n",
        "---\n",
        "\n",
        "# 9. Numerical Gradient Checking\n",
        "\n",
        "Use finite differences:\n",
        "\n",
        "âˆ‚L/âˆ‚Î¸ â‰ˆ (L(Î¸ + Îµ) âˆ’ L(Î¸ âˆ’ Îµ)) / (2Îµ)\n",
        "\n",
        "Compare:\n",
        "- Analytical gradient\n",
        "- Numerical gradient\n",
        "\n",
        "If difference < 1e-6 â†’ correct.\n",
        "\n",
        "This is mandatory before training.\n",
        "\n",
        "---\n",
        "\n",
        "# Coding Exercises\n",
        "\n",
        "## Question 1: Conv Backward (Single Channel)\n",
        "\n",
        "Implement:\n",
        "- dKernel\n",
        "- dInput\n",
        "\n",
        "Test:\n",
        "- Small input (5Ã—5)\n",
        "- Small kernel (3Ã—3)\n",
        "- Compare with numerical gradients\n",
        "\n",
        "---\n",
        "\n",
        "## Question 2: Multi-Channel Conv Backward\n",
        "\n",
        "Extend to:\n",
        "- Multiple filters\n",
        "- Multiple input channels\n",
        "\n",
        "Verify shapes carefully.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 3: Max Pool Backward\n",
        "\n",
        "Implement:\n",
        "- Store argmax during forward\n",
        "- Route gradient correctly\n",
        "\n",
        "Test on small example.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 4: Build Full CNN Backward\n",
        "\n",
        "Chain together:\n",
        "- Conv backward\n",
        "- ReLU backward\n",
        "- Pool backward\n",
        "- FC backward\n",
        "\n",
        "Test gradient consistency.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: Train CNN on MNIST (Small Subset)\n",
        "\n",
        "Steps:\n",
        "- Load 5k samples\n",
        "- Train simple CNN\n",
        "- Use Adam optimizer\n",
        "- Track loss & accuracy\n",
        "\n",
        "Goal:\n",
        "- Reach >90% accuracy on small subset\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: Compare with MLP\n",
        "\n",
        "Train MLP with similar parameter count.\n",
        "\n",
        "Compare:\n",
        "- Accuracy\n",
        "- Convergence speed\n",
        "- Generalization\n",
        "\n",
        "Plot:\n",
        "- Train loss\n",
        "- Validation accuracy\n",
        "\n",
        "---\n",
        "\n",
        "# Conceptual Questions\n",
        "\n",
        "1. Why must kernel be rotated for input gradient?\n",
        "2. Why is gradient accumulation necessary?\n",
        "3. Why does max pooling backward require stored indices?\n",
        "4. Why is numerical gradient checking important?\n",
        "5. Why do CNNs generalize better than MLPs on images?\n",
        "\n",
        "---\n",
        "\n",
        "# Outcome of Week 10\n",
        "\n",
        "After this week, you should:\n",
        "- Understand convolution backprop deeply\n",
        "- Trust your gradient implementation\n",
        "- Train CNN end-to-end\n",
        "- Appreciate inductive bias mathematically\n"
      ],
      "metadata": {
        "id": "IIcMS79lqOgJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DWS9ogMHujoP",
        "outputId": "9ad3d0c8-bd70-4107-e3d2-249e6d43c876"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (26.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.1.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.78.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.46.3)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32s24Rf0p-t4",
        "outputId": "ba66aafd-9ddc-4467-b0a3-3de534678892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
            "(5000, 28, 28)\n",
            "(5000,)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "\n",
        "X_train = X_train[:5000]\n",
        "y_train = y_train[:5000]\n",
        "\n",
        "X_test = X_test[:1000]\n",
        "y_test = y_test[:1000]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 1 â€” Single Channel Convolution Backward**\n",
        "\n",
        "We start small:\n",
        "\n",
        "Input: 5Ã—5\n",
        "\n",
        "Kernel: 3Ã—3\n",
        "\n",
        "Stride = 1\n",
        "\n",
        "No padding"
      ],
      "metadata": {
        "id": "7ivLB83Yuvt1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 1 â€” Conv Backward (Single Channel)**\n",
        "\n",
        "\n",
        "Forward"
      ],
      "metadata": {
        "id": "XiuGc8tXu0V8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_forward(X, K):\n",
        "    H, W = X.shape\n",
        "    k = K.shape[0]\n",
        "\n",
        "    out_h = H - k + 1\n",
        "    out_w = W - k + 1\n",
        "\n",
        "    out = np.zeros((out_h, out_w))\n",
        "\n",
        "    for i in range(out_h):\n",
        "        for j in range(out_w):\n",
        "            out[i,j] = np.sum(X[i:i+k, j:j+k] * K)\n",
        "\n",
        "    cache = (X, K)\n",
        "    return out, cache\n"
      ],
      "metadata": {
        "id": "G2vCGhfYuzuI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward\n",
        "\n",
        "**dKernel**\n",
        "\n",
        "ğ‘‘ğ¾(ğ‘š,ğ‘›)=âˆ‘ğ‘‘ğ‘‚ğ‘¢ğ‘¡(ğ‘–,ğ‘—)âˆ—ğ‘‹(ğ‘–+ğ‘š,ğ‘—+ğ‘›)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "**dInput**\n",
        "\n",
        "ğ‘‘ğ‘‹=ğ‘ğ‘œğ‘›ğ‘£(ğ‘‘ğ‘‚ğ‘¢ğ‘¡,ğ‘Ÿğ‘œğ‘¡ğ‘ğ‘¡ğ‘’ğ‘‘(ğ¾))\n"
      ],
      "metadata": {
        "id": "AdHYcjLpvmxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_backward(dout, cache):\n",
        "    X, K = cache\n",
        "    H, W = X.shape\n",
        "    k = K.shape[0]\n",
        "\n",
        "    dK = np.zeros_like(K)\n",
        "    dX = np.zeros_like(X)\n",
        "\n",
        "    # dKernel\n",
        "    for m in range(k):\n",
        "        for n in range(k):\n",
        "            for i in range(dout.shape[0]):\n",
        "                for j in range(dout.shape[1]):\n",
        "                    dK[m,n] += dout[i,j] * X[i+m, j+n]\n",
        "\n",
        "    # Rotate kernel 180Â°\n",
        "    K_rot = np.rot90(K, 2)\n",
        "\n",
        "    # Pad dout for full convolution\n",
        "    pad = k - 1\n",
        "    dout_padded = np.pad(dout, ((pad,pad),(pad,pad)))\n",
        "\n",
        "    # dInput\n",
        "    for i in range(H):\n",
        "        for j in range(W):\n",
        "            dX[i,j] = np.sum(\n",
        "                dout_padded[i:i+k, j:j+k] * K_rot\n",
        "            )\n",
        "\n",
        "    return dX, dK\n"
      ],
      "metadata": {
        "id": "7eGrxph_wkt-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ğŸ”¹ Numerical Gradient Check (MANDATORY)"
      ],
      "metadata": {
        "id": "XQUtGmXr1UBd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient(X, K, eps=1e-5):\n",
        "    dK_num = np.zeros_like(K)\n",
        "\n",
        "    for m in range(K.shape[0]):\n",
        "        for n in range(K.shape[1]):\n",
        "            K_pos = K.copy()\n",
        "            K_neg = K.copy()\n",
        "\n",
        "            K_pos[m,n] += eps\n",
        "            K_neg[m,n] -= eps\n",
        "\n",
        "            out_pos, _ = conv_forward(X, K_pos)\n",
        "            out_neg, _ = conv_forward(X, K_neg)\n",
        "\n",
        "            loss_pos = np.sum(out_pos)\n",
        "            loss_neg = np.sum(out_neg)\n",
        "\n",
        "            dK_num[m,n] = (loss_pos - loss_neg) / (2*eps)\n",
        "\n",
        "    return dK_num\n"
      ],
      "metadata": {
        "id": "-GYaLfND1U30"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.random.randn(5,5)\n",
        "K = np.random.randn(3,3)\n",
        "\n",
        "out, cache = conv_forward(X,K)\n",
        "dout = np.ones_like(out)\n",
        "\n",
        "dX, dK = conv_backward(dout, cache)\n",
        "dK_num = numerical_gradient(X,K)\n",
        "\n",
        "print(\"Difference:\", np.linalg.norm(dK - dK_num))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2IVQEi32LN5",
        "outputId": "8413aa08-ffec-4795-f9f4-e548f352448b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference: 2.0921302088569912e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 3 â€” Max Pool Backward**\n",
        "\n",
        "Forward (store argmax)"
      ],
      "metadata": {
        "id": "zKYAKkWHjNPP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxpool_forward(X, k = 2):\n",
        "  H, W = X.shape\n",
        "  out_h = H // k\n",
        "  out_w = W // k\n",
        "\n",
        "  out = np.zeros((out_h, out_w))\n",
        "  mask = {}\n",
        "\n",
        "  for i in range(out_h):\n",
        "    for j in range(out_w):\n",
        "      window = X[i*k:(i+1)*k, j*k:(j+1)*k]\n",
        "      max_val = np.max(window)\n",
        "      out[i, j] = max_val\n",
        "\n",
        "      idx = np.unravel_index(np.argmax(window), window.shape)\n",
        "      mask[(i,j)] = (i*k+idx[0], j*k+idx[1])\n",
        "\n",
        "  cache = (X, mask)\n",
        "  return out, cache"
      ],
      "metadata": {
        "id": "G6FIeDJSjPs9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Backward"
      ],
      "metadata": {
        "id": "_bIMbYvKkWms"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def maxpool_backward(dout, cache, k=2):\n",
        "  X, mask = cache\n",
        "  dX = np.zeros_like(X)\n",
        "\n",
        "  for i in range(dout.shape[0]):\n",
        "    for j in range(dout.shape[1]):\n",
        "      x_idx, y_idx = mask[(i, j)]\n",
        "      dX[x_idx, y_idx] = dout[i, j]\n",
        "\n",
        "  return dX"
      ],
      "metadata": {
        "id": "2i5nML-UkXrD"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU Backward"
      ],
      "metadata": {
        "id": "SBIAl_J8k4gf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def relu_forward(X):\n",
        "  out = np.maximum(0, X)\n",
        "  cache = X\n",
        "  return out, cache\n",
        "\n",
        "\n",
        "def relu_backward(dout, cache):\n",
        "  X = cache\n",
        "  dX = dout.copy()\n",
        "  dX[X <= 0] = 0\n",
        "  return dX"
      ],
      "metadata": {
        "id": "gS7V1Z01k5AO"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 2 â€” Build CNN With Backward\n",
        "\n",
        "Weâ€™ll build:\n",
        "\n",
        "Conv â†’ ReLU â†’ Pool â†’ FC â†’ Softmax"
      ],
      "metadata": {
        "id": "NXJLvjK9lQOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "FC Layer"
      ],
      "metadata": {
        "id": "Tjnm0S14lVZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FC:\n",
        "  def __init__(self, in_dim, out_dim):\n",
        "    self.W = np.random.randn(out_dim, in_dim) * 0.01\n",
        "    self.b = np.zeros(out_dim)\n",
        "\n",
        "  def forward(self, X):\n",
        "    self.X = X\n",
        "    return self.W @ self.X + self.b\n",
        "\n",
        "  def backward(self, dout):\n",
        "    dW = np.outer(dout, self.X)\n",
        "    db = dout\n",
        "    dX = self.W.T @ dout\n",
        "    return dX, dW, db"
      ],
      "metadata": {
        "id": "XVmG4lDBlVE0"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax + Cross Entropy"
      ],
      "metadata": {
        "id": "U1GVHnMPl_Wz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def softmax(x):\n",
        "  e = np.exp(x - np.max(x))\n",
        "  return e / np.sum(e)\n",
        "\n",
        "def cross_entropy(probs, y):\n",
        "  return -np.log(probs[y])"
      ],
      "metadata": {
        "id": "yrwsdAWKlS13"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Full Training Loop (Small CNN)\n",
        "\n",
        "For simplicity:\n",
        "\n",
        "1 conv filter\n",
        "\n",
        "1 pooling\n",
        "\n",
        "1 FC"
      ],
      "metadata": {
        "id": "e_whkWK9mUGy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "K = np.random.randn(3,3)*0.1\n",
        "fc = FC(13*13,10)\n",
        "\n",
        "lr = 0.01\n",
        "\n",
        "for epoch in range(3):\n",
        "    total_loss = 0\n",
        "\n",
        "    for i in range(1000):\n",
        "        X = X_train[i]\n",
        "        y = y_train[i]\n",
        "\n",
        "        # Forward\n",
        "        conv_out, conv_cache = conv_forward(X,K)\n",
        "        relu_out, relu_cache = relu_forward(conv_out)\n",
        "        pool_out, pool_cache = maxpool_forward(relu_out)\n",
        "\n",
        "        flat = pool_out.flatten()\n",
        "        logits = fc.forward(flat)\n",
        "        probs = softmax(logits)\n",
        "\n",
        "        loss = cross_entropy(probs,y)\n",
        "        total_loss += loss\n",
        "\n",
        "        # Backward\n",
        "        dlogits = probs\n",
        "        dlogits[y] -= 1\n",
        "\n",
        "        dflat, dW, db = fc.backward(dlogits)\n",
        "\n",
        "        dpool = dflat.reshape(pool_out.shape)\n",
        "        drelu = maxpool_backward(dpool, pool_cache)\n",
        "        dconv = relu_backward(drelu, relu_cache)\n",
        "\n",
        "        dX, dK = conv_backward(dconv, conv_cache)\n",
        "\n",
        "        # Update\n",
        "        K -= lr * dK\n",
        "        fc.W -= lr * dW\n",
        "        fc.b -= lr * db\n",
        "\n",
        "    print(\"Epoch:\", epoch, \"Loss:\", total_loss/1000)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgJcBYzTmTwT",
        "outputId": "877dbf48-c6f1-46f0-c4bc-0b2ea911fec0"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0 Loss: 1.5987798797476802\n",
            "Epoch: 1 Loss: 0.5543139232454839\n",
            "Epoch: 2 Loss: 0.4209488095590292\n"
          ]
        }
      ]
    }
  ]
}