{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi3y5iCDFftga2+b4Z2Mxi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kunalavghade/Ai/blob/main/Week_12_%E2%80%94_Recurrent_Neural_Networks_%26_Backpropagation_Through_Time.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Week 12 ‚Äî Recurrent Neural Networks & Backpropagation Through Time\n",
        "\n",
        "## Goal\n",
        "\n",
        "Understand how neural networks process sequences and learn temporal dependencies.\n",
        "\n",
        "By the end of this week, you should:\n",
        "- Implement a vanilla RNN cell from scratch\n",
        "- Understand hidden state dynamics\n",
        "- Implement Backpropagation Through Time (BPTT)\n",
        "- Predict simple sequences\n",
        "- Observe exploding/vanishing gradients\n",
        "\n",
        "This week introduces time into neural networks.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Why Sequential Models?\n",
        "\n",
        "Unlike images:\n",
        "- Text\n",
        "- Time series\n",
        "- Speech\n",
        "- Sensor signals\n",
        "\n",
        "Have temporal dependency.\n",
        "\n",
        "Order matters.\n",
        "\n",
        "Example:\n",
        "\"The movie was not good\"\n",
        "\n",
        "Meaning depends on sequence.\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Vanilla RNN Architecture\n",
        "\n",
        "At time step t:\n",
        "\n",
        "h_t = tanh(W_x x_t + W_h h_{t-1} + b)\n",
        "\n",
        "y_t = W_y h_t + c\n",
        "\n",
        "Where:\n",
        "- x_t = input at time t\n",
        "- h_t = hidden state\n",
        "- W_x = input weights\n",
        "- W_h = recurrent weights\n",
        "- W_y = output weights\n",
        "\n",
        "The hidden state stores memory.\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Key Idea: Parameter Sharing Across Time\n",
        "\n",
        "Same weights:\n",
        "- Used at every time step\n",
        "\n",
        "This is like:\n",
        "- Deep network unrolled over time\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Unrolling Through Time\n",
        "\n",
        "Sequence length T:\n",
        "\n",
        "x1 ‚Üí h1 ‚Üí h2 ‚Üí h3 ‚Üí ... ‚Üí hT\n",
        "\n",
        "This is equivalent to:\n",
        "- A deep network of depth T\n",
        "- With shared weights\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Backpropagation Through Time (BPTT)\n",
        "\n",
        "To compute gradients:\n",
        "- Unroll network\n",
        "- Apply chain rule across time\n",
        "\n",
        "Gradient at time t depends on:\n",
        "- All future losses\n",
        "\n",
        "This leads to:\n",
        "\n",
        "‚àÇL/‚àÇW_h = Œ£ over time\n",
        "\n",
        "---\n",
        "\n",
        "# 6. Vanishing & Exploding Gradients in RNNs\n",
        "\n",
        "Repeated multiplication:\n",
        "\n",
        "‚àÇh_t/‚àÇh_{t-1} = W_h * derivative(tanh)\n",
        "\n",
        "If eigenvalues < 1 ‚Üí vanishing  \n",
        "If eigenvalues > 1 ‚Üí exploding  \n",
        "\n",
        "This problem is worse than in deep CNNs.\n",
        "\n",
        "---\n",
        "\n",
        "# 7. Truncated BPTT\n",
        "\n",
        "Instead of backprop through entire sequence:\n",
        "- Backprop only k steps\n",
        "\n",
        "Benefits:\n",
        "- Reduces computation\n",
        "- Stabilizes training\n",
        "\n",
        "Tradeoff:\n",
        "- May miss long-term dependencies\n",
        "\n",
        "---\n",
        "\n",
        "# 8. Sequence Prediction Setup\n",
        "\n",
        "Two common tasks:\n",
        "\n",
        "1. Many-to-one:\n",
        "   - Input sequence ‚Üí single output\n",
        "\n",
        "2. Many-to-many:\n",
        "   - Input sequence ‚Üí output at each time step\n",
        "\n",
        "---\n",
        "\n",
        "# Coding Exercises\n",
        "\n",
        "## Question 1: Implement RNN Cell\n",
        "\n",
        "Implement forward step:\n",
        "\n",
        "h_t = tanh(W_x x_t + W_h h_prev + b)\n",
        "\n",
        "Return:\n",
        "- h_t\n",
        "- cache for backward\n",
        "\n",
        "---\n",
        "\n",
        "## Question 2: Full Forward Pass\n",
        "\n",
        "Given sequence of length T:\n",
        "- Loop over time\n",
        "- Store hidden states\n",
        "\n",
        "Test with:\n",
        "- Random data\n",
        "\n",
        "---\n",
        "\n",
        "## Question 3: Backprop Through Time\n",
        "\n",
        "Implement:\n",
        "- Gradients w.r.t W_x\n",
        "- Gradients w.r.t W_h\n",
        "- Gradients w.r.t b\n",
        "\n",
        "Accumulate gradients over time.\n",
        "\n",
        "Verify with numerical gradient checking.\n",
        "\n",
        "---\n",
        "\n",
        "## Question 4: Predict Number Sequence\n",
        "\n",
        "Train RNN on:\n",
        "- Input: [1,2,3,4]\n",
        "- Target: [2,3,4,5]\n",
        "\n",
        "Or simple sine wave.\n",
        "\n",
        "Observe:\n",
        "- Learning behavior\n",
        "- Gradient norms\n",
        "\n",
        "---\n",
        "\n",
        "## Question 5: Character-Level Toy Dataset\n",
        "\n",
        "Create small dataset:\n",
        "- \"hello world\"\n",
        "Train RNN to:\n",
        "- Predict next character\n",
        "\n",
        "Observe:\n",
        "- Generated sequences\n",
        "- Training stability\n",
        "\n",
        "---\n",
        "\n",
        "## Question 6: Implement Truncated BPTT\n",
        "\n",
        "Modify training:\n",
        "- Backprop only k steps (e.g., k=5)\n",
        "\n",
        "Compare:\n",
        "- Full BPTT vs truncated\n",
        "- Stability\n",
        "- Speed\n",
        "\n",
        "---\n",
        "\n",
        "# Conceptual Questions\n",
        "\n",
        "1. Why does unrolling create a deep network?\n",
        "2. Why do gradients vanish faster in RNNs?\n",
        "3. Why does tanh contribute to vanishing gradients?\n",
        "4. Why is truncated BPTT useful?\n",
        "5. Why is sequence length critical?\n",
        "\n",
        "---\n",
        "\n",
        "# Outcome of Week 12\n",
        "\n",
        "After this week, you should:\n",
        "- Understand RNN mechanics deeply\n",
        "- Implement BPTT from scratch\n",
        "- See exploding gradients firsthand\n",
        "- Feel the limitations of vanilla RNNs"
      ],
      "metadata": {
        "id": "C5c3Vu_1zLGP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "GJOyjxPczIUC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "üîπ PART 1 ‚Äî RNN Cell (Single Time Step)\n",
        "\n",
        "We implement:\n",
        "  Hùë° = tanh(ùëäùë•Xùë° +ùëä‚ÑéHùë°‚àí1+ùëè)\n",
        "\t‚Äã"
      ],
      "metadata": {
        "id": "imda3E-rYcBD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**‚úÖ Question 1 ‚Äî RNN Cell Forward**"
      ],
      "metadata": {
        "id": "NYhgv0fYY4XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNCell:\n",
        "  def __init__(self, input_dim, hidden_dim):\n",
        "    self.input_dim = input_dim\n",
        "    self.hidden_dim = hidden_dim\n",
        "\n",
        "    self.Wx = np.random.randn(hidden_dim, input_dim) * 0.1\n",
        "    self.Wh = np.random.randn(hidden_dim, hidden_dim) * 0.1\n",
        "    self.b = np.zeros((hidden_dim, 1))\n",
        "\n",
        "  def forward(self, x_t, h_prev):\n",
        "    z = self.Wx @ x_t + self.Wh @ h_prev + self.b\n",
        "    h_t = np.tanh(z)\n",
        "    cache = (x_t, h_prev, z, h_t)\n",
        "    return h_t, cache"
      ],
      "metadata": {
        "id": "CVNvfyRZzN0c"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test:"
      ],
      "metadata": {
        "id": "3P4lZAPHZqGC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cell = RNNCell(3,5)\n",
        "\n",
        "x = np.random.randn(3,1)\n",
        "h_prev = np.random.randn(5,1)\n",
        "\n",
        "h_t, cache = cell.forward(x, h_prev)\n",
        "\n",
        "print(h_t.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxrkrX8YZsTA",
        "outputId": "3df3c019-f20e-4f1d-88a8-5a3faca6f414"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "üîπ **PART 2 ‚Äî Full Forward Through Time**"
      ],
      "metadata": {
        "id": "U9V2Z5UeZ2-5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "‚úÖ Question 2 ‚Äî Sequence Forward"
      ],
      "metadata": {
        "id": "WOHDq-KiZ_HN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_forward(cell, X_seq):\n",
        "  h_prev = np.zeros((cell.hidden_dim, 1))\n",
        "\n",
        "  caches, ha_states = [], []\n",
        "  for x_t in X_seq:\n",
        "    h_prev, cache = cell.forward(x_t, h_prev)\n",
        "    caches.append(cache)\n",
        "    ha_states.append(h_prev)\n",
        "  return ha_states, caches"
      ],
      "metadata": {
        "id": "4PfOuz3IZ-0d"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test:"
      ],
      "metadata": {
        "id": "bHKWdZMYaU8A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_seq = [np.random.randn(3, 1) for _ in range(4)]\n",
        "h_states, caches = rnn_forward(cell, X_seq)\n",
        "\n",
        "print(len(h_states))\n",
        "print(h_states[0].shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N58nRaEWZ6h2",
        "outputId": "2ae7b00b-e4a0-42f0-b71a-2d8b44b796f3"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n",
            "(5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PART 3 ‚Äî Backpropagation Through Time (BPTT)**\n",
        "\n",
        "‚úÖ **Question 3 ‚Äî RNN Backward (BPTT)**\n",
        "\n",
        "We compute gradients for:\n",
        "- Wx\n",
        "- Wh\n",
        "- b\n",
        "\n",
        "Key equation:\n",
        "\n",
        "$\\frac{\\partial L}{\\partial h_t} =\n",
        "\\frac{\\partial L_t}{\\partial h_t} +\n",
        "W_h^{T} \\frac{\\partial L}{\\partial h_{t+1}}$\n",
        "\n",
        "## Meaning (Quick Intuition)\n",
        "\n",
        "- $L$ ‚Üí total loss  \n",
        "\n",
        "- $L_t$ ‚Üí loss at time step $t$  \n",
        "\n",
        "- $h_t$ ‚Üí hidden state at time $t$  \n",
        "\n",
        "- $W_h$ ‚Üí recurrent weight matrix  \n",
        "\n",
        "---\n",
        "\n",
        "### What This Equation Tells Us\n",
        "\n",
        "It shows that the gradient at time $t$ depends on:\n",
        "\n",
        "1. **The direct loss at time $t$**\n",
        "   \n",
        "   $$\n",
        "   \\frac{\\partial L_t}{\\partial h_t}\n",
        "   $$\n",
        "\n",
        "2. **The gradient flowing back from time $t+1$**\n",
        "   \n",
        "   $$\n",
        "   W_h^T \\frac{\\partial L}{\\partial h_{t+1}}\n",
        "   $$\n",
        "\n",
        "---\n",
        "\n",
        "### Intuition in Words\n",
        "\n",
        "When computing gradients in an RNN:\n",
        "\n",
        "- The hidden state $h_t$ affects:\n",
        "  - The loss at the current time step.\n",
        "  - Future hidden states $h_{t+1}, h_{t+2}, \\dots$\n",
        "\n",
        "- So its gradient must accumulate:\n",
        "  - Immediate contribution.\n",
        "  - Future contribution passed backward through the recurrent weights.\n",
        "\n",
        "This recursive dependency is what leads to vanishing or exploding gradients in long sequences.\n",
        "\n",
        "This is the recursive gradient accumulation."
      ],
      "metadata": {
        "id": "odZsJEVIZ6Jh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_backward(cell, dh_list, caches):\n",
        "  \"\"\"\n",
        "    dh_list: list of dL/dh_t for each time step\n",
        "  \"\"\"\n",
        "  dWx = np.zeros_like(cell.Wx)\n",
        "  dWh = np.zeros_like(cell.Wh)\n",
        "  db = np.zeros_like(cell.b)\n",
        "\n",
        "  dh_next = np.zeros((cell.hidden_dim, 1))\n",
        "  for t in reversed(range(len(caches))):\n",
        "    x_t, h_prev, z, h_t = caches[t]\n",
        "    dh = dh_list[t] + dh_next\n",
        "\n",
        "    dz = dh * (1 - h_t**2) # tanh derivative\n",
        "\n",
        "    dWx += dz @ x_t.T\n",
        "    dWh += dz @ h_prev.T\n",
        "    db += dz\n",
        "\n",
        "    dh_next = cell.Wh.T @ dz\n",
        "\n",
        "  return dWx, dWh, db"
      ],
      "metadata": {
        "id": "oWyLurA6bhB0"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Numerical Gradient Check"
      ],
      "metadata": {
        "id": "UrVkduQVdhHT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def numerical_gradient_Wx(cell, X_seq, eps=1e-5):\n",
        "  grad = np.zeros_like(cell.Wx)\n",
        "\n",
        "  for i in range(cell.Wx.shape[0]):\n",
        "    for j in range(cell.Wx.shape[1]):\n",
        "      original = cell.Wx[i, j]\n",
        "\n",
        "      cell.Wx[i, j] = original + eps\n",
        "      h_plus, _ = rnn_forward(cell, X_seq)\n",
        "      loss_plus = sum(np.sum(h) for h in h_plus)\n",
        "\n",
        "\n",
        "      cell.Wx[i, j] = original - eps\n",
        "      h_minus, _ = rnn_forward(cell, X_seq)\n",
        "      loss_minus = sum(np.sum(h) for h in h_minus)\n",
        "\n",
        "      grad[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
        "      cell.Wx[i, j] = original\n",
        "  return grad"
      ],
      "metadata": {
        "id": "u34nobyddjFb"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test:"
      ],
      "metadata": {
        "id": "V8QY32ZQi1lS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cell = RNNCell(2,3)\n",
        "X_seq = [np.random.randn(2,1) for _ in range(3)]\n",
        "\n",
        "h_states, caches = rnn_forward(cell, X_seq)\n",
        "dh_list = [np.ones_like(h) for h in h_states]\n",
        "\n",
        "dWx, dWh, db = rnn_backward(cell, dh_list, caches)\n",
        "dWx_num = numerical_gradient_Wx(cell, X_seq)\n",
        "\n",
        "print(\"Difference:\", np.linalg.norm(dWx - dWx_num))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RW7xaZKJi3FF",
        "outputId": "e828cf59-f72e-46de-96ef-acab4b3e848f"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Difference: 2.836222449079511e-10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 4 ‚Äî Train RNN on Number Sequence\n",
        "\n",
        "Task:\n",
        "\n",
        "Input: [1,2,3,4...100]\n",
        "\n",
        "Target: [2,3,4,5...100]"
      ],
      "metadata": {
        "id": "lMTu4TS4e_ej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sequence(start, length):\n",
        "  X, Y = [],[]\n",
        "  for i in range(length):\n",
        "    X.append(np.array([[start + i]]))\n",
        "    Y.append(np.array([[start + i + 1]]))\n",
        "  return X, Y\n"
      ],
      "metadata": {
        "id": "aXp48CkTfH5c"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add Output Layer"
      ],
      "metadata": {
        "id": "sb1MyrlZfHe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel:\n",
        "  def __init__(self):\n",
        "    self.cell = RNNCell(1, 10)\n",
        "    self.Wy = np.random.randn(1, 10) * 0.1\n",
        "    self.by = np.zeros((1,1))\n",
        "\n",
        "  def forward(self, X_seq):\n",
        "    h_states, caches = rnn_forward(self.cell, X_seq)\n",
        "    outputs = []\n",
        "    for h in h_states:\n",
        "      output = self.Wy @ h + self.by\n",
        "      outputs.append(output)\n",
        "    return outputs, h_states, caches"
      ],
      "metadata": {
        "id": "CLkMr4wzflPr"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training Loop"
      ],
      "metadata": {
        "id": "N1CvX_NIgmY1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RNNModel()\n",
        "lr = 0.001\n",
        "X_seq, Y_seq = generate_sequence(1, 10)\n",
        "print(X_seq)\n",
        "print(Y_seq)\n",
        "\n",
        "losses = []\n",
        "logging = True\n",
        "epochs = [ i for i in range(5000)]\n",
        "\n",
        "grad_clip = 1.0 # Define gradient clipping threshold\n",
        "\n",
        "for epoch in epochs:\n",
        "  outputs, h_states, caches = model.forward(X_seq)\n",
        "  loss, dh_list = 0, []\n",
        "\n",
        "  for y_pred, y_true, h in zip(outputs, Y_seq, h_states):\n",
        "    loss += np.sum((y_pred - y_true)**2)\n",
        "    dy = 2 * (y_pred - y_true)\n",
        "    dh = model.Wy.T @ dy\n",
        "    dh_list.append(dh)\n",
        "\n",
        "    # Apply gradient clipping for output layer weights\n",
        "    model.Wy -= lr * np.clip(dy @ h.T, -grad_clip, grad_clip)\n",
        "    model.by -= lr * np.clip(dy, -grad_clip, grad_clip)\n",
        "\n",
        "  dWx, dWh, db = rnn_backward(model.cell, dh_list, caches)\n",
        "\n",
        "  # Apply gradient clipping for RNN cell weights\n",
        "  model.cell.Wx -= lr * np.clip(dWx, -grad_clip, grad_clip)\n",
        "  model.cell.Wh -= lr * np.clip(dWh, -grad_clip, grad_clip)\n",
        "  model.cell.b -= lr * np.clip(db, -grad_clip, grad_clip)\n",
        "\n",
        "  losses.append(loss)\n",
        "  if epoch % 100 == 0 and logging:\n",
        "    print(f\"Epoch {epoch}, Loss: {loss}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c_CWrdx9gnB1",
        "outputId": "36f13eb3-1ee9-4a7a-e4f9-c01c8f847434"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([[1]]), array([[2]]), array([[3]]), array([[4]]), array([[5]]), array([[6]]), array([[7]]), array([[8]]), array([[9]]), array([[10]])]\n",
            "[array([[2]]), array([[3]]), array([[4]]), array([[5]]), array([[6]]), array([[7]]), array([[8]]), array([[9]]), array([[10]]), array([[11]])]\n",
            "Epoch 0, Loss: 490.7886741409609\n",
            "Epoch 100, Loss: 29.238136182196502\n",
            "Epoch 200, Loss: 46.274723643485245\n",
            "Epoch 300, Loss: 52.30624251792246\n",
            "Epoch 400, Loss: 54.9779100398943\n",
            "Epoch 500, Loss: 56.573447268372256\n",
            "Epoch 600, Loss: 57.48037253660326\n",
            "Epoch 700, Loss: 58.084664333685154\n",
            "Epoch 800, Loss: 58.562102678633735\n",
            "Epoch 900, Loss: 58.957611453945276\n",
            "Epoch 1000, Loss: 59.294006780229545\n",
            "Epoch 1100, Loss: 59.58584660131686\n",
            "Epoch 1200, Loss: 59.842985280798274\n",
            "Epoch 1300, Loss: 60.07238848895922\n",
            "Epoch 1400, Loss: 60.27915284669405\n",
            "Epoch 1500, Loss: 60.46711408091531\n",
            "Epoch 1600, Loss: 60.63922758229825\n",
            "Epoch 1700, Loss: 60.79781617413503\n",
            "Epoch 1800, Loss: 60.94473689812552\n",
            "Epoch 1900, Loss: 61.08149652884167\n",
            "Epoch 2000, Loss: 61.20933357064631\n",
            "Epoch 2100, Loss: 61.32927772609396\n",
            "Epoch 2200, Loss: 61.4421938492512\n",
            "Epoch 2300, Loss: 61.54881498204138\n",
            "Epoch 2400, Loss: 61.64976756079918\n",
            "Epoch 2500, Loss: 61.74559091023728\n",
            "Epoch 2600, Loss: 61.836752504733845\n",
            "Epoch 2700, Loss: 61.92366004932279\n",
            "Epoch 2800, Loss: 62.00667114052011\n",
            "Epoch 2900, Loss: 62.08610106389061\n",
            "Epoch 3000, Loss: 62.16222914171629\n",
            "Epoch 3100, Loss: 62.2353039412741\n",
            "Epoch 3200, Loss: 62.305547579559914\n",
            "Epoch 3300, Loss: 62.37315930541904\n",
            "Epoch 3400, Loss: 62.43831849925783\n",
            "Epoch 3500, Loss: 62.50118719988042\n",
            "Epoch 3600, Loss: 62.56191224476384\n",
            "Epoch 3700, Loss: 62.62062709230469\n",
            "Epoch 3800, Loss: 62.67745338084942\n",
            "Epoch 3900, Loss: 62.73250226864072\n",
            "Epoch 4000, Loss: 62.785875590444036\n",
            "Epoch 4100, Loss: 62.83766686000807\n",
            "Epoch 4200, Loss: 62.887962142263476\n",
            "Epoch 4300, Loss: 62.936840814960846\n",
            "Epoch 4400, Loss: 62.98437623607178\n",
            "Epoch 4500, Loss: 63.03063633054045\n",
            "Epoch 4600, Loss: 63.07568410774978\n",
            "Epoch 4700, Loss: 63.119578119246974\n",
            "Epoch 4800, Loss: 63.162372864778355\n",
            "Epoch 4900, Loss: 63.20411915345095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Gradient norm:\", np.linalg.norm(dWh))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VIDSQUxkZ9C",
        "outputId": "c1dd55f9-35dd-436d-eaaf-89d6d886ad4f"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient norm: 0.015477958438056636\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "767e225e",
        "outputId": "66f42985-3729-43d1-f558-45e2fdb41485"
      },
      "source": [
        "# Make a prediction with the trained model\n",
        "input_value = np.array([[3]]) # Example input\n",
        "\n",
        "# The model.forward expects a list of inputs, even for a single time step\n",
        "outputs, _, _ = model.forward([input_value])\n",
        "\n",
        "print(f\"Input: {input_value[0][0]}\")\n",
        "print(f\"Predicted output: {outputs[0][0][0]:.4f}\")"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: 3\n",
            "Predicted output: 6.0233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 5 ‚Äî Character Level Toy Dataset"
      ],
      "metadata": {
        "id": "0FioW4yNoSFl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"hello world\"\n",
        "chars = list(set(text))\n",
        "char_to_ix = {ch:i for i,ch in enumerate(chars)}\n",
        "ix_to_char = {i:ch for ch,i in char_to_ix.items()}"
      ],
      "metadata": {
        "id": "MTBTi6i-oTex"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(idx, size):\n",
        "    vec = np.zeros((size,1))\n",
        "    vec[idx] = 1\n",
        "    return vec"
      ],
      "metadata": {
        "id": "9mIvGjIxoWiy"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "PART 6 ‚Äî Truncated BPTT\n",
        "\n",
        "Modify backward loop:\n",
        "\n",
        "Instead of:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "for t in reversed(range(len(caches))):\n",
        "```\n",
        "\n",
        "```\n",
        "for t in reversed(range(max(0, len(caches)-k), len(caches))):\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "gpT5X-hNoZCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def rnn_backward(cell, dh_list, caches):\n",
        "  \"\"\"\n",
        "    dh_list: list of dL/dh_t for each time step\n",
        "  \"\"\"\n",
        "  k = 5\n",
        "  dWx = np.zeros_like(cell.Wx)\n",
        "  dWh = np.zeros_like(cell.Wh)\n",
        "  db = np.zeros_like(cell.b)\n",
        "\n",
        "  dh_next = np.zeros((cell.hidden_dim, 1))\n",
        "  for t in reversed(range(max(0, len(caches)-k), len(caches))):\n",
        "    x_t, h_prev, z, h_t = caches[t]\n",
        "    dh = dh_list[t] + dh_next\n",
        "\n",
        "    dz = dh * (1 - h_t**2) # tanh derivative\n",
        "\n",
        "    dWx += dz @ x_t.T\n",
        "    dWh += dz @ h_prev.T\n",
        "    db += dz\n",
        "\n",
        "    dh_next = cell.Wh.T @ dz\n",
        "\n",
        "  return dWx, dWh, db"
      ],
      "metadata": {
        "id": "32XIEzs1ocDw"
      },
      "execution_count": 93,
      "outputs": []
    }
  ]
}